{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c476d3ce",
   "metadata": {},
   "source": [
    "# CH09_2_Generating Recurrent Network and Classifying Text\n",
    "\n",
    "- Last update : 2022.04.14. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59039d9d",
   "metadata": {},
   "source": [
    "## # Prepare training set and verifying set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "85ff4aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Loading the IMDB data set from tensorflow \n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "(x_train_all, y_train_all), (x_test, y_test) = imdb.load_data(skip_top=20, num_words=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c54acb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,) (25000,)\n"
     ]
    }
   ],
   "source": [
    "# 2. Cheking out training set size\n",
    "print (x_train_all.shape, y_train_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf32bc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 22, 2, 43, 2, 2, 2, 2, 65, 2, 2, 66, 2, 2, 2, 36, 2, 2, 25, 2, 43, 2, 2, 50, 2, 2, 2, 35, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 39, 2, 2, 2, 2, 2, 2, 38, 2, 2, 2, 2, 50, 2, 2, 2, 2, 2, 2, 22, 2, 2, 2, 2, 2, 22, 71, 87, 2, 2, 43, 2, 38, 76, 2, 2, 2, 2, 22, 2, 2, 2, 2, 2, 2, 2, 2, 2, 62, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 66, 2, 33, 2, 2, 2, 2, 38, 2, 2, 25, 2, 51, 36, 2, 48, 25, 2, 33, 2, 22, 2, 2, 28, 77, 52, 2, 2, 2, 2, 82, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 36, 71, 43, 2, 2, 26, 2, 2, 46, 2, 2, 2, 2, 2, 2, 88, 2, 2, 2, 2, 98, 32, 2, 56, 26, 2, 2, 2, 2, 2, 2, 2, 22, 21, 2, 2, 26, 2, 2, 2, 30, 2, 2, 51, 36, 28, 2, 92, 25, 2, 2, 2, 65, 2, 38, 2, 88, 2, 2, 2, 2, 2, 2, 2, 2, 32, 2, 2, 2, 2, 2, 32]\n"
     ]
    }
   ],
   "source": [
    "# 3. Checking out Training set sample\n",
    "\n",
    "print (x_train_all[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "143b7a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22, 43, 65, 66, 36, 25, 43, 50, 35, 39, 38, 50, 22, 22, 71, 87, 43, 38, 76, 22, 62, 66, 33, 38, 25, 51, 36, 48, 25, 33, 22, 28, 77, 52, 82, 36, 71, 43, 26, 46, 88, 98, 32, 56, 26, 22, 21, 26, 30, 51, 36, 28, 92, 25, 65, 38, 88, 32, 32]\n"
     ]
    }
   ],
   "source": [
    "# 4. Eliminating 2 from training set \n",
    "\n",
    "for i in range(len(x_train_all)):\n",
    "    x_train_all[i] = [w for w in x_train_all[i] if w > 2]\n",
    "    \n",
    "print (x_train_all[0])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb7069c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 0s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. downloading terminology dictionary\n",
    "\n",
    "word_to_index = imdb.get_word_index()\n",
    "word_to_index['movie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f204afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "film just story really they you just there an from so there film film were great just so much film would really at so you what they if you at film have been good also they were just are out because them all up are film but are be what they have don't you story so because all all "
     ]
    }
   ],
   "source": [
    "# 6. Transforming integer in training set into English words string\n",
    "\n",
    "index_to_word = {word_to_index[k]: k for k in word_to_index}\n",
    "\n",
    "for w in x_train_all[0]: \n",
    "    print(index_to_word[w - 3], end=' ')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19980d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59 32\n"
     ]
    }
   ],
   "source": [
    "# 7. Checking out the length of the training sample\n",
    "\n",
    "print(len(x_train_all[0]), len(x_train_all[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aee88cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list([22, 43, 65, 66, 36, 25, 43, 50, 35, 39, 38, 50, 22, 22, 71, 87, 43, 38, 76, 22, 62, 66, 33, 38, 25, 51, 36, 48, 25, 33, 22, 28, 77, 52, 82, 36, 71, 43, 26, 46, 88, 98, 32, 56, 26, 22, 21, 26, 30, 51, 36, 28, 92, 25, 65, 38, 88, 32, 32])\n",
      " list([78, 26, 20, 21, 69, 30, 23, 93, 35, 89, 29, 46, 37, 45, 43, 38, 26, 68, 98, 43, 50, 32, 78, 22, 64, 23, 28, 52, 33, 89, 78, 95])\n",
      " list([47, 30, 31, 54, 61, 71, 22, 33, 75, 43, 86, 35, 33, 89, 78, 66, 58, 43, 85, 42, 83, 68, 36, 36, 69, 22, 28, 40, 87, 23, 21, 23, 22, 40, 57, 31, 22, 47, 51, 23, 79, 89, 35])\n",
      " ...\n",
      " list([45, 84, 21, 84, 84, 36, 28, 57, 21, 84, 56, 31, 20, 97, 20, 53, 74, 29, 45, 40, 29, 89, 70, 29, 64, 26, 27, 47, 84, 37, 61, 34, 65, 59])\n",
      " list([69, 72, 23, 54, 45, 58, 43, 23, 62, 30, 51, 32, 61, 71, 66, 75, 37, 69, 75, 44, 69, 50, 23, 40, 40, 25, 70, 31, 62, 40, 25, 52, 58, 92, 39, 38, 84, 80, 23])\n",
      " list([22, 45, 39, 50, 47, 38, 24, 78, 21, 27, 92, 42, 97, 90, 35, 29, 27, 97, 21, 66, 78, 21, 60, 27, 43, 40, 20, 72, 51, 22])]\n"
     ]
    }
   ],
   "source": [
    "print(x_train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf32c91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 1 0 0 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# 8. Checking out target data of training set \n",
    "\n",
    "print(y_train_all[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "73d5affd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> x_train (20000,)=\n",
      " [list([35, 40, 27, 28, 40, 22, 83, 31, 85, 45, 24, 23, 31, 70, 31, 76, 30, 98, 32, 22, 28, 51, 75, 56, 30, 33, 97, 53, 38, 46, 53, 74, 31, 35, 23, 34, 22, 58])\n",
      " list([54, 39, 27, 35, 35, 56, 27, 29, 80, 27, 27, 46, 23, 35, 64, 86, 65, 35, 22, 63, 73, 29, 22, 82, 34, 50, 26, 21, 47, 30, 97, 83, 76, 69, 86, 31, 61, 49, 99, 85, 85, 26, 73, 81, 87, 53, 26, 53, 74, 26, 53, 62, 28, 21, 50, 44, 93, 22, 39, 34, 21, 45, 87, 20, 32])\n",
      " list([26, 42, 99, 30, 78, 35, 96, 45, 48, 88, 41, 24, 59, 43, 23, 61, 20, 30, 42, 33, 32])\n",
      " ...\n",
      " list([39, 76, 56, 34, 94, 64, 39, 67, 82, 66, 94, 30, 23, 38, 50, 26, 54, 25, 70, 67, 34, 57, 23, 50, 26, 26, 26, 56, 48, 25, 79, 32, 97, 78, 20, 94, 55, 67, 49, 26, 21, 64, 25, 26, 31, 91, 94, 22, 47, 49, 94, 24, 67, 21, 94, 66, 48, 25, 81, 81, 38, 24])\n",
      " list([20, 67, 62, 30, 35, 54, 25, 79, 20, 69, 55, 20, 63, 71, 23, 70, 33, 32, 67, 20, 80, 24])\n",
      " list([22, 66, 76, 45, 55, 63, 63, 62, 60, 49, 45, 56, 54, 36, 71, 71, 23, 42, 21, 51, 50, 26, 52, 37, 38, 76, 40, 68, 22, 36, 97, 68, 45, 66, 39, 37, 28, 45, 65, 37, 46, 34, 27, 56, 27, 49])]\n",
      "\n",
      ">>> y_train(20000,) = [0 1 0 ... 0 1 1]\n",
      "-------------------------------\n",
      "\n",
      ">>> x_val (5000,)=\n",
      " [list([55, 27, 49, 34, 49, 26, 77, 20, 47, 82, 25, 53, 54, 29, 31])\n",
      " list([76, 21, 22, 47, 87, 68, 43, 73, 43, 55, 71, 24, 78, 49, 84, 32, 26, 71, 69, 65, 40, 71, 21, 61, 63, 64, 85, 79, 54, 86, 32, 32, 22, 23])\n",
      " list([44, 68, 71, 21, 53, 53, 40, 93, 46, 66, 82, 20, 39, 22, 20, 73, 26, 44, 99, 22, 33, 49, 50, 73, 20, 66, 46, 39, 49, 31, 21, 66, 78, 31, 20, 52, 94, 73, 94, 65, 82, 45, 20, 23, 20, 63, 20, 94, 65, 31, 20, 94, 20, 80, 84, 88, 36, 30, 55, 55, 20, 25, 66, 92, 28, 30, 83, 20, 45, 20, 94, 20, 82, 93, 34, 94, 37, 92, 69, 28, 20, 21, 26, 68, 20, 49, 52, 52, 20, 23, 94])\n",
      " ...\n",
      " list([97, 20, 44, 28, 23, 73, 48, 25, 63, 39, 69, 20, 39, 27, 60, 74, 29, 64, 22, 48, 25, 70, 44, 41, 38, 73, 23, 41, 36, 41, 41, 38, 44, 22, 21, 60, 44, 22, 72, 37, 20, 39, 73, 33, 90, 29, 66, 99, 46, 27, 21, 48, 44, 27, 42, 56, 73, 43, 33, 46])\n",
      " list([61, 88, 20, 61, 86, 20, 33, 49, 93, 72, 23, 20, 51, 25])\n",
      " list([20, 20, 86, 58, 23, 24, 69, 24, 60, 23, 33, 86, 30, 22, 67, 80, 20, 25, 25, 80, 76, 59, 80, 25])]\n",
      "\n",
      ">>> y_val (5000,)= [0 1 1 ... 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# 9. preparing verifying dataset\n",
    "\n",
    "np.random.seed(42)\n",
    "random_index = np.random.permutation(25000)\n",
    "\n",
    "x_train = x_train_all[random_index[:20000]]\n",
    "y_train = y_train_all[random_index[:20000]]\n",
    "x_val = x_train_all[random_index[20000:]]\n",
    "y_val = y_train_all[random_index[20000:]]\n",
    "\n",
    "print (\">>> x_train \"+str(x_train.shape)+\"=\\n\", x_train)\n",
    "print (\"\\n>>> y_train\"+str(y_train.shape)+\" =\", y_train)\n",
    "print ('-------------------------------')\n",
    "print (\"\\n>>> x_val \"+str(x_val.shape)+\"=\\n\", x_val)\n",
    "print (\"\\n>>> y_val \"+str(y_val.shape)+\"=\", y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82146f70",
   "metadata": {},
   "source": [
    "## # Alligning the length of sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5ac5ba5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Aligning the length of sample using tensorflow\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence \n",
    "\n",
    "maxlen = 100 \n",
    "x_train_seq = sequence.pad_sequences(x_train, maxlen = maxlen)\n",
    "x_val_seq = sequence.pad_sequences(x_val, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4b44d58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> x_train_seq.shape, x_val_seq.shape = (20000, 100) (5000, 100)\n",
      "\n",
      ">>> x_train_seq[0] =\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0 35 40 27 28 40 22 83 31 85 45\n",
      " 24 23 31 70 31 76 30 98 32 22 28 51 75 56 30 33 97 53 38 46 53 74 31 35\n",
      " 23 34 22 58]\n"
     ]
    }
   ],
   "source": [
    "# 2. Checking out the size of alligned training set and sample \n",
    "\n",
    "print (\">>> x_train_seq.shape, x_val_seq.shape =\", x_train_seq.shape, x_val_seq.shape)\n",
    "\n",
    "print ('\\n>>> x_train_seq[0] =\\n', x_train_seq[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1877b222",
   "metadata": {},
   "source": [
    "## # One-hot Incoding of the samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a1469e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> x_train_onehot.shape = (20000, 100, 100)\n",
      ">>> x_train_onehot.nbytes =  800000000 Byte\n",
      ">>> x_train_onehot.nbytes =    762.939 MB\n"
     ]
    }
   ],
   "source": [
    "# 1. One-hot incoding using tensorflow and cheking out the size of variables \n",
    "\n",
    "from tensorflow.keras.utils import to_categorical \n",
    "\n",
    "x_train_onehot = to_categorical(x_train_seq)\n",
    "x_val_onehot = to_categorical(x_val_seq)\n",
    "\n",
    "# 1.1. size checking \n",
    "print (\">>> x_train_onehot.shape =\", x_train_onehot.shape)\n",
    "\n",
    "# 1.2. memory checking \n",
    "print (\">>> x_train_onehot.nbytes = {0:10d} Byte\".format(x_train_onehot.nbytes))\n",
    "print (\">>> x_train_onehot.nbytes = {0:10.3f} MB\".format(x_train_onehot.nbytes/1024 /1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e0385d",
   "metadata": {},
   "source": [
    "## # Making Recurrent network class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bd8fd37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Modifying __init__() method\n",
    "\n",
    "def __init__(self, n_cells=10, batch_size=32, learning_rate=0.1):\n",
    "    self.n_cells = n_cells            # no. of cells\n",
    "    self.batch_size = batch_size      # batch size \n",
    "    self.w1h = None                   # weight of hidden status \n",
    "    self.w1x = None                   # weight of input \n",
    "    self.b1 = None                    # interscept of recurrent layer \n",
    "    self.w2 = None                    # weight of output layer\n",
    "    self.b2 = None                    # interscept of output layer\n",
    "    self.h = None                     # activation output of recurrent layer \n",
    "    self.losses = []                  # training loss \n",
    "    self.val_losses = []              # verification loss\n",
    "    self.lf = learning_rate           # learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "212107a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Initializing weight using orthogonal initialization \n",
    "\n",
    "def init_weights(self, n_features, n_classes):\n",
    "    orth_init = tf.initializers.Orthogonal()\n",
    "    glorot_init = tf.initializers.GlorotUniform()\n",
    "    \n",
    "    self.w1h = orth_init((self.n_cells, self.n_cells)).numpy()  # (no. of cells, no. of cells)\n",
    "    self.w1x = glorot_init((n_features, self.n_cells)).numpy()  # (no. of features, no. of cells)\n",
    "    self.b1 = np.zeros(self.n_cells)                            # size of hidden layer \n",
    "    self.w2 = glorot_init((self.n_cells, n_classes)).numpy()    # (no. of cells, no. of classes)\n",
    "    self.b2 = np.zeros(n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8c7a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Constructing Forward Propagation calculation \n",
    "\n",
    "def forpass(self, x): \n",
    "    self.h = [np.zeros((x.shape[0], self.n_cells))]    # initializing hidden status \n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b527d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Changing the batch dimension and time steps\n",
    "    ...\n",
    "    # Exchange the batch dimension with step dimension \n",
    "    seq = np.swapaxes(x, 0, 1)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b3fa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. forward propagation calculation in every time steps of each sample\n",
    "    ...\n",
    "    # calculating recurrent layer linear regression \n",
    "    for x in seq: \n",
    "        z1 = np.dot(x, self.w1x)+np.dot(self.h[-1], self.w1h) + self.b1\n",
    "        h = np.tanh(z1)                           # applying activation function \n",
    "        self.h.append(h)                          # saving hidden status for backpropagation \n",
    "        z2 = np.dot(h, self.w2) + self.b2         # calculating linear regression of output layer\n",
    "    return z2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f5cbb3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Implementing backward propagation calculation \n",
    "\n",
    "def backpro(self, x, err): \n",
    "    m = len(x)              # no. of samples \n",
    "    \n",
    "    # calculating gradient of weight and interscept of output layer \n",
    "    w2_grad = np.dot(self.h[-1].T, err) / m\n",
    "    b2_grad = np.sum(err) / m \n",
    "    \n",
    "    # exchange batch dimension with time step dimension \n",
    "    seq = np.swapaxes(x, 0, 1)\n",
    "    \n",
    "    w1h_grad = w1x_grad = b1_grad = 0 \n",
    "    \n",
    "    # calculating gradient right before cell \n",
    "    err_to_cell = np.dot(err, self.w2.T) * (1-self.h[-1]**2)\n",
    "    \n",
    "    # transfer gradient by backpropagating every time step\n",
    "    for x, h in zip(seq[::-1][:10], self.h[:-1][::-1][:10]):\n",
    "        w1h_grad += np.dot(h.T, err_to_cell)\n",
    "        w1x_grad += np.dot(x.T, err_to_cell)\n",
    "        b1_grad += np.sum(err_to_cell, axis=0)\n",
    "        \n",
    "        err_to_cell = np.dot(err_to_cell, self.w1h) * (1-h**2)\n",
    "        \n",
    "    w1h_grad /= m\n",
    "    w1x_grad /= m\n",
    "    b1_grad /= m\n",
    "     \n",
    "    return w1h_grad, w1x_grad, b1_grad, w2_grad, b2_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b6cc5ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Modifying other methods\n",
    "\n",
    "class RecurrentNetwork:\n",
    "    \n",
    "    def __init__(self, n_cells=10, batch_size=32, learning_rate=0.1):\n",
    "        self.n_cells = n_cells            # no. of cells\n",
    "        self.batch_size = batch_size      # batch size \n",
    "        self.w1h = None                   # weight of hidden status \n",
    "        self.w1x = None                   # weight of input \n",
    "        self.b1 = None                    # interscept of recurrent layer \n",
    "        self.w2 = None                    # weight of output layer\n",
    "        self.b2 = None                    # interscept of output layer\n",
    "        self.h = None                     # activation output of recurrent layer \n",
    "        self.losses = []                  # training loss \n",
    "        self.val_losses = []              # verification loss\n",
    "        self.lr = learning_rate           # learning rate \n",
    "        \n",
    "    def forpass(self, x): \n",
    "        self.h = [np.zeros((x.shape[0], self.n_cells))]    # initializing hidden status \n",
    "        # Exchange the batch dimension with step dimension \n",
    "        seq = np.swapaxes(x, 0, 1)\n",
    "        # calculating recurrent layer linear regression \n",
    "        for x in seq: \n",
    "            z1 = np.dot(x, self.w1x) + np.dot(self.h[-1], self.w1h) + self.b1\n",
    "            h = np.tanh(z1)                           # applying activation function \n",
    "            self.h.append(h)                          # saving hidden status for backpropagation \n",
    "            z2 = np.dot(h, self.w2) + self.b2         # calculating linear regression of output layer\n",
    "        return z2 \n",
    "    \n",
    "    def backprop(self, x, err): \n",
    "        m = len(x)              # no. of samples \n",
    "\n",
    "        # calculating gradient of weight and interscept of output layer \n",
    "        w2_grad = np.dot(self.h[-1].T, err) / m\n",
    "        b2_grad = np.sum(err) / m \n",
    "        # exchange batch dimension with time step dimension \n",
    "        seq = np.swapaxes(x, 0, 1)\n",
    "\n",
    "        w1h_grad = w1x_grad = b1_grad = 0 \n",
    "        # calculating gradient right before cell \n",
    "        err_to_cell = np.dot(err, self.w2.T) * (1-self.h[-1]**2)\n",
    "        # transfer gradient by backpropagating every time step\n",
    "        for x, h in zip(seq[::-1][:10], self.h[:-1][::-1][:10]):\n",
    "            w1h_grad += np.dot(h.T, err_to_cell)\n",
    "            w1x_grad += np.dot(x.T, err_to_cell)\n",
    "            b1_grad += np.sum(err_to_cell, axis=0)\n",
    "            # calculating gradient right before the cell in previous time step\n",
    "            err_to_cell = np.dot(err_to_cell, self.w1h) * (1-h**2)\n",
    "\n",
    "        w1h_grad /= m\n",
    "        w1x_grad /= m\n",
    "        b1_grad /= m\n",
    "\n",
    "        return w1h_grad, w1x_grad, b1_grad, w2_grad, b2_grad\n",
    "    \n",
    "    def sigmoid(self, z): \n",
    "        a = 1/ (1+np.exp(-z))                 # calculating sigmoid \n",
    "        return a \n",
    "    \n",
    "    def init_weights(self, n_features, n_classes): \n",
    "        orth_init = tf.initializers.Orthogonal()\n",
    "        glorot_init = tf.initializers.GlorotUniform()\n",
    "        \n",
    "        self.w1h = orth_init((self.n_cells, self.n_cells)).numpy() # (no. of cells, no. of cells)\n",
    "        self.w1x = glorot_init((n_features, self.n_cells)).numpy() # (no. of features, no. of cells)\n",
    "        self.b1 = np.zeros(self.n_cells)                           # size of hidden layer  \n",
    "        self.w2 = glorot_init((self.n_cells, n_classes)).numpy()  # (no. of cells, no. of classes)\n",
    "        self.b2 = np.zeros(n_classes)\n",
    "        \n",
    "    def fit(self, x, y, epochs=100, x_val=None, y_val=None):\n",
    "        y = y.reshape(-1, 1)\n",
    "        y_val = y_val.reshape(-1, 1)\n",
    "        np.random.seed(42)\n",
    "        self.init_weights(x.shape[2], y.shape[1])    # initializing weight of hidden layer and output layer \n",
    "        # repeate the routine epoch times \n",
    "        for i in range(epochs): \n",
    "            print (\"EPOCH\", i , end=' ')\n",
    "            # circulating mini-batch returned from generator function \n",
    "            batch_losses = []\n",
    "            for x_batch, y_batch in self.gen_batch(x, y): \n",
    "                print ('.', end ='')\n",
    "                a = self.training(x_batch, y_batch)    \n",
    "                \n",
    "                # clipping for safe logarian calculation \n",
    "                a = np.clip(a, 1e-10, 1-1e-10)\n",
    "                # accumulating logarian loss and regularization loss to the loss and adding it to the list\n",
    "                loss = np.mean(-(y_batch*np.log(a)+(1-y_batch)*np.log(1-a)))\n",
    "                batch_losses.append(loss)\n",
    "            print ( )\n",
    "            self.losses.append(np.mean(batch_losses))\n",
    "            # calculating the loss of verification set \n",
    "            self.update_val_loss(x_val, y_val)\n",
    "    \n",
    "    # mini-batch generator function \n",
    "    def gen_batch(self, x, y): \n",
    "        length = len(x)\n",
    "        bins = length // self.batch_size      # mini batch frequency \n",
    "        if length % self.batch_size: \n",
    "            bins +=1                          # if it has remainder        \n",
    "        indexes = np.random.permutation(np.arange(len(x)))   # shuffle indexes \n",
    "        x = x[indexes]\n",
    "        y = y[indexes]\n",
    "        for i in range(bins):\n",
    "            start = self.batch_size * i \n",
    "            end = self.batch_size * (i + 1)\n",
    "            yield x[start:end], y[start:end]      # returning the value slicing in batch_size                                \n",
    "        \n",
    "    \n",
    "    def training(self, x, y):\n",
    "        m = len(x)                            # saving the no. of samples\n",
    "        z = self.forpass(x)                   # carring on forward propagation calculation \n",
    "        a = self.sigmoid(z)                   # applying activation function \n",
    "        err = -(y - a)                        # calculating errors \n",
    "        # Calculating gradient by backpropagating errors\n",
    "        w1h_grad, w1x_grad, b1_grad, w2_grad, b2_grad = self.backprop(x, err)\n",
    "        # updating weight and interscept of cell \n",
    "        self.w1h -= self.lr * w1h_grad \n",
    "        self.w1x -= self.lr * w1x_grad \n",
    "        self.b1 -= self.lr * b1_grad\n",
    "        # updating weight and interscept of output layers \n",
    "        self.w2 -= self.lr *  w2_grad \n",
    "        self.b2 -= self.lr * b2_grad\n",
    "        return a \n",
    "        \n",
    "    def predict(self, x):\n",
    "        z = self.forpass(x)            # carry opn forward propagation calculation \n",
    "        return z > 0                   # applying step function \n",
    "    \n",
    "    def score(self, x, y):\n",
    "        # returning True ratio by comparing prediction with target column vector\n",
    "        return np.mean(self.predict(x) == y.reshape(-1, 1))    \n",
    "    \n",
    "    def update_val_loss(self, x_val, y_val):\n",
    "        z = self.forpass(x_val)           # carry opn forward propagation calculation \n",
    "        a = self.sigmoid(z)               # applying activation function \n",
    "        a = np.clip(a, 1e-10, 1-1e-10)    # clipping the output value \n",
    "        val_loss = np.mean(-(y_val*np.log(a) + (1-y_val)*np.log(1-a)))\n",
    "        self.val_losses.append(val_loss)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051b7d94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c298667",
   "metadata": {},
   "source": [
    "## # Model Training of Recurrent Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "54eed7e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "EPOCH 1 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "EPOCH 2 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "EPOCH 3 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "EPOCH 4 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "EPOCH 5 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "EPOCH 6 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "EPOCH 7 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "EPOCH 8 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "EPOCH 9 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "EPOCH 10 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "EPOCH 11 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 12 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "EPOCH 13 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "EPOCH 14 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "EPOCH 15 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "EPOCH 16 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "EPOCH 17 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "EPOCH 18 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "EPOCH 19 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n"
     ]
    }
   ],
   "source": [
    "# 1. Model Training of Recurrent Network \n",
    "\n",
    "rn = RecurrentNetwork(n_cells=32, batch_size=32, learning_rate=0.01)\n",
    "\n",
    "rn.fit(x_train_onehot, y_train, epochs=20, x_val=x_val_onehot, y_val=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7fcc6aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. Plotting the Loss graph on the training and verifying set\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(rn.losses)\n",
    "plt.plot(rn.val_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1cffb1ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Evaluating accuracy of verification set \n",
    "\n",
    "rn.score(x_val_onehot, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2790a2e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49736a4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow_py37",
   "language": "python",
   "name": "tf_py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
