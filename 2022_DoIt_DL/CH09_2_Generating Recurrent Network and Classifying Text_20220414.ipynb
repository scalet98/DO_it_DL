{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c476d3ce",
   "metadata": {},
   "source": [
    "# CH09_2_Generating Recurrent Network and Classifying Text\n",
    "\n",
    "- Last update : 2022.04.14. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59039d9d",
   "metadata": {},
   "source": [
    "## # Prepare training set and verifying set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85ff4aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Loading the IMDB data set from tensorflow \n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "(x_train_all, y_train_all), (x_test, y_test) = imdb.load_data(skip_top=20, num_words=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c54acb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,) (25000,)\n"
     ]
    }
   ],
   "source": [
    "# 2. Cheking out training set size\n",
    "print (x_train_all.shape, y_train_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf32bc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 22, 2, 43, 2, 2, 2, 2, 65, 2, 2, 66, 2, 2, 2, 36, 2, 2, 25, 2, 43, 2, 2, 50, 2, 2, 2, 35, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 39, 2, 2, 2, 2, 2, 2, 38, 2, 2, 2, 2, 50, 2, 2, 2, 2, 2, 2, 22, 2, 2, 2, 2, 2, 22, 71, 87, 2, 2, 43, 2, 38, 76, 2, 2, 2, 2, 22, 2, 2, 2, 2, 2, 2, 2, 2, 2, 62, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 66, 2, 33, 2, 2, 2, 2, 38, 2, 2, 25, 2, 51, 36, 2, 48, 25, 2, 33, 2, 22, 2, 2, 28, 77, 52, 2, 2, 2, 2, 82, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 36, 71, 43, 2, 2, 26, 2, 2, 46, 2, 2, 2, 2, 2, 2, 88, 2, 2, 2, 2, 98, 32, 2, 56, 26, 2, 2, 2, 2, 2, 2, 2, 22, 21, 2, 2, 26, 2, 2, 2, 30, 2, 2, 51, 36, 28, 2, 92, 25, 2, 2, 2, 65, 2, 38, 2, 88, 2, 2, 2, 2, 2, 2, 2, 2, 32, 2, 2, 2, 2, 2, 32]\n"
     ]
    }
   ],
   "source": [
    "# 3. Checking out Training set sample\n",
    "\n",
    "print (x_train_all[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "143b7a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22, 43, 65, 66, 36, 25, 43, 50, 35, 39, 38, 50, 22, 22, 71, 87, 43, 38, 76, 22, 62, 66, 33, 38, 25, 51, 36, 48, 25, 33, 22, 28, 77, 52, 82, 36, 71, 43, 26, 46, 88, 98, 32, 56, 26, 22, 21, 26, 30, 51, 36, 28, 92, 25, 65, 38, 88, 32, 32]\n"
     ]
    }
   ],
   "source": [
    "# 4. Eliminating 2 from training set \n",
    "\n",
    "for i in range(len(x_train_all)):\n",
    "    x_train_all[i] = [w for w in x_train_all[i] if w > 2]\n",
    "    \n",
    "print (x_train_all[0])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb7069c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. downloading terminology dictionary\n",
    "\n",
    "word_to_index = imdb.get_word_index()\n",
    "word_to_index['movie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f204afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "film just story really they you just there an from so there film film were great just so much film would really at so you what they if you at film have been good also they were just are out because them all up are film but are be what they have don't you story so because all all "
     ]
    }
   ],
   "source": [
    "# 6. Transforming integer in training set into English words string\n",
    "\n",
    "index_to_word = {word_to_index[k]: k for k in word_to_index}\n",
    "\n",
    "for w in x_train_all[0]: \n",
    "    print(index_to_word[w - 3], end=' ')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19980d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59 32\n"
     ]
    }
   ],
   "source": [
    "# 7. Checking out the length of the training sample\n",
    "\n",
    "print(len(x_train_all[0]), len(x_train_all[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aee88cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list([22, 43, 65, 66, 36, 25, 43, 50, 35, 39, 38, 50, 22, 22, 71, 87, 43, 38, 76, 22, 62, 66, 33, 38, 25, 51, 36, 48, 25, 33, 22, 28, 77, 52, 82, 36, 71, 43, 26, 46, 88, 98, 32, 56, 26, 22, 21, 26, 30, 51, 36, 28, 92, 25, 65, 38, 88, 32, 32])\n",
      " list([78, 26, 20, 21, 69, 30, 23, 93, 35, 89, 29, 46, 37, 45, 43, 38, 26, 68, 98, 43, 50, 32, 78, 22, 64, 23, 28, 52, 33, 89, 78, 95])\n",
      " list([47, 30, 31, 54, 61, 71, 22, 33, 75, 43, 86, 35, 33, 89, 78, 66, 58, 43, 85, 42, 83, 68, 36, 36, 69, 22, 28, 40, 87, 23, 21, 23, 22, 40, 57, 31, 22, 47, 51, 23, 79, 89, 35])\n",
      " ...\n",
      " list([45, 84, 21, 84, 84, 36, 28, 57, 21, 84, 56, 31, 20, 97, 20, 53, 74, 29, 45, 40, 29, 89, 70, 29, 64, 26, 27, 47, 84, 37, 61, 34, 65, 59])\n",
      " list([69, 72, 23, 54, 45, 58, 43, 23, 62, 30, 51, 32, 61, 71, 66, 75, 37, 69, 75, 44, 69, 50, 23, 40, 40, 25, 70, 31, 62, 40, 25, 52, 58, 92, 39, 38, 84, 80, 23])\n",
      " list([22, 45, 39, 50, 47, 38, 24, 78, 21, 27, 92, 42, 97, 90, 35, 29, 27, 97, 21, 66, 78, 21, 60, 27, 43, 40, 20, 72, 51, 22])]\n"
     ]
    }
   ],
   "source": [
    "print(x_train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf32c91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 1 0 0 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# 8. Checking out target data of training set \n",
    "\n",
    "print(y_train_all[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73d5affd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> x_train (20000,)=\n",
      " [list([35, 40, 27, 28, 40, 22, 83, 31, 85, 45, 24, 23, 31, 70, 31, 76, 30, 98, 32, 22, 28, 51, 75, 56, 30, 33, 97, 53, 38, 46, 53, 74, 31, 35, 23, 34, 22, 58])\n",
      " list([54, 39, 27, 35, 35, 56, 27, 29, 80, 27, 27, 46, 23, 35, 64, 86, 65, 35, 22, 63, 73, 29, 22, 82, 34, 50, 26, 21, 47, 30, 97, 83, 76, 69, 86, 31, 61, 49, 99, 85, 85, 26, 73, 81, 87, 53, 26, 53, 74, 26, 53, 62, 28, 21, 50, 44, 93, 22, 39, 34, 21, 45, 87, 20, 32])\n",
      " list([26, 42, 99, 30, 78, 35, 96, 45, 48, 88, 41, 24, 59, 43, 23, 61, 20, 30, 42, 33, 32])\n",
      " ...\n",
      " list([39, 76, 56, 34, 94, 64, 39, 67, 82, 66, 94, 30, 23, 38, 50, 26, 54, 25, 70, 67, 34, 57, 23, 50, 26, 26, 26, 56, 48, 25, 79, 32, 97, 78, 20, 94, 55, 67, 49, 26, 21, 64, 25, 26, 31, 91, 94, 22, 47, 49, 94, 24, 67, 21, 94, 66, 48, 25, 81, 81, 38, 24])\n",
      " list([20, 67, 62, 30, 35, 54, 25, 79, 20, 69, 55, 20, 63, 71, 23, 70, 33, 32, 67, 20, 80, 24])\n",
      " list([22, 66, 76, 45, 55, 63, 63, 62, 60, 49, 45, 56, 54, 36, 71, 71, 23, 42, 21, 51, 50, 26, 52, 37, 38, 76, 40, 68, 22, 36, 97, 68, 45, 66, 39, 37, 28, 45, 65, 37, 46, 34, 27, 56, 27, 49])]\n",
      "\n",
      ">>> y_train(20000,) = [0 1 0 ... 0 1 1]\n",
      "-------------------------------\n",
      "\n",
      ">>> x_val (5000,)=\n",
      " [list([55, 27, 49, 34, 49, 26, 77, 20, 47, 82, 25, 53, 54, 29, 31])\n",
      " list([76, 21, 22, 47, 87, 68, 43, 73, 43, 55, 71, 24, 78, 49, 84, 32, 26, 71, 69, 65, 40, 71, 21, 61, 63, 64, 85, 79, 54, 86, 32, 32, 22, 23])\n",
      " list([44, 68, 71, 21, 53, 53, 40, 93, 46, 66, 82, 20, 39, 22, 20, 73, 26, 44, 99, 22, 33, 49, 50, 73, 20, 66, 46, 39, 49, 31, 21, 66, 78, 31, 20, 52, 94, 73, 94, 65, 82, 45, 20, 23, 20, 63, 20, 94, 65, 31, 20, 94, 20, 80, 84, 88, 36, 30, 55, 55, 20, 25, 66, 92, 28, 30, 83, 20, 45, 20, 94, 20, 82, 93, 34, 94, 37, 92, 69, 28, 20, 21, 26, 68, 20, 49, 52, 52, 20, 23, 94])\n",
      " ...\n",
      " list([97, 20, 44, 28, 23, 73, 48, 25, 63, 39, 69, 20, 39, 27, 60, 74, 29, 64, 22, 48, 25, 70, 44, 41, 38, 73, 23, 41, 36, 41, 41, 38, 44, 22, 21, 60, 44, 22, 72, 37, 20, 39, 73, 33, 90, 29, 66, 99, 46, 27, 21, 48, 44, 27, 42, 56, 73, 43, 33, 46])\n",
      " list([61, 88, 20, 61, 86, 20, 33, 49, 93, 72, 23, 20, 51, 25])\n",
      " list([20, 20, 86, 58, 23, 24, 69, 24, 60, 23, 33, 86, 30, 22, 67, 80, 20, 25, 25, 80, 76, 59, 80, 25])]\n",
      "\n",
      ">>> y_val (5000,)= [0 1 1 ... 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# 9. preparing verifying dataset\n",
    "\n",
    "np.random.seed(42)\n",
    "random_index = np.random.permutation(25000)\n",
    "\n",
    "x_train = x_train_all[random_index[:20000]]\n",
    "y_train = y_train_all[random_index[:20000]]\n",
    "x_val = x_train_all[random_index[20000:]]\n",
    "y_val = y_train_all[random_index[20000:]]\n",
    "\n",
    "print (\">>> x_train \"+str(x_train.shape)+\"=\\n\", x_train)\n",
    "print (\"\\n>>> y_train\"+str(y_train.shape)+\" =\", y_train)\n",
    "print ('-------------------------------')\n",
    "print (\"\\n>>> x_val \"+str(x_val.shape)+\"=\\n\", x_val)\n",
    "print (\"\\n>>> y_val \"+str(y_val.shape)+\"=\", y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82146f70",
   "metadata": {},
   "source": [
    "## # Alligning the length of sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ac5ba5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Aligning the length of sample using tensorflow\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence \n",
    "\n",
    "maxlen = 100 \n",
    "x_train_seq = sequence.pad_sequences(x_train, maxlen = maxlen)\n",
    "x_val_seq = sequence.pad_sequences(x_val, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b44d58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> x_train_seq.shape, x_val_seq.shape = (20000, 100) (5000, 100)\n",
      "\n",
      ">>> x_train_seq[0] =\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0 35 40 27 28 40 22 83 31 85 45\n",
      " 24 23 31 70 31 76 30 98 32 22 28 51 75 56 30 33 97 53 38 46 53 74 31 35\n",
      " 23 34 22 58]\n"
     ]
    }
   ],
   "source": [
    "# 2. Checking out the size of alligned training set and sample \n",
    "\n",
    "print (\">>> x_train_seq.shape, x_val_seq.shape =\", x_train_seq.shape, x_val_seq.shape)\n",
    "\n",
    "print ('\\n>>> x_train_seq[0] =\\n', x_train_seq[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1877b222",
   "metadata": {},
   "source": [
    "## # One-hot Incoding of the samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1469e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> x_train_onehot.shape = (20000, 100, 100)\n",
      ">>> x_train_onehot.nbytes =  800000000 Byte\n",
      ">>> x_train_onehot.nbytes =    762.939 MB\n"
     ]
    }
   ],
   "source": [
    "# 1. One-hot incoding using tensorflow and cheking out the size of variables \n",
    "\n",
    "from tensorflow.keras.utils import to_categorical \n",
    "\n",
    "x_train_onehot = to_categorical(x_train_seq)\n",
    "x_val_onehot = to_categorical(x_val_seq)\n",
    "\n",
    "# 1.1. size checking \n",
    "print (\">>> x_train_onehot.shape =\", x_train_onehot.shape)\n",
    "\n",
    "# 1.2. memory checking \n",
    "print (\">>> x_train_onehot.nbytes = {0:10d} Byte\".format(x_train_onehot.nbytes))\n",
    "print (\">>> x_train_onehot.nbytes = {0:10.3f} MB\".format(x_train_onehot.nbytes/1024 /1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e0385d",
   "metadata": {},
   "source": [
    "## # Making Recurrent network class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd8fd37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Modifying __init__() method\n",
    "\n",
    "def __init__(self, n_cells=10, batch_size=32, learning_rate=0.1):\n",
    "    self.n_cells = n_cells            # no. of cells\n",
    "    self.batch_size = batch_size      # batch size \n",
    "    self.w1h = None                   # weight of hidden status \n",
    "    self.w1x = None                   # weight of input \n",
    "    self.b1 = None                    # interscept of recurrent layer \n",
    "    self.w2 = None                    # weight of output layer\n",
    "    self.b2 = None                    # interscept of output layer\n",
    "    self.h = None                     # activation output of recurrent layer \n",
    "    self.losses = []                  # training loss \n",
    "    self.val_losses = []              # verification loss\n",
    "    self.lf = learning_rate           # learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "212107a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Initializing weight using orthogonal initialization \n",
    "\n",
    "def init_weights(self, n_features, n_classes):\n",
    "    orth_init = tf.initializers.Orthogonal()\n",
    "    glorot_init = tf.initializers.GlorotUniform()\n",
    "    \n",
    "    self.w1h = orth_init((self.n_cells, self.n_cells)).numpy()  # (no. of cells, no. of cells)\n",
    "    self.w1x = glorot_init((n_features, self.n_cells)).numpy()  # (no. of features, no. of cells)\n",
    "    self.b1 = np.zeros(self.n_cells)                            # size of hidden layer \n",
    "    self.w2 = glorot_init((self.n_cells, n_classes)).numpy()    # (no. of cells, no. of classes)\n",
    "    self.b2 = np.zeros(n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf8c7a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Constructing Forward Propagation calculation \n",
    "\n",
    "def forpass(self, x): \n",
    "    self.h = [np.zeros((x.shape[0], self.n_cells))]    # initializing hidden status \n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b527d10",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (Temp/ipykernel_1476/2733641252.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\scalet98\\AppData\\Local\\Temp/ipykernel_1476/2733641252.py\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    ...\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# 4. Changing the batch dimension and time steps\n",
    "    ...\n",
    "    # Exchange the batch dimension with step dimension \n",
    "    seq = np.swapaxes(x, 0, 1)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88b3fa51",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (Temp/ipykernel_1476/196572691.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\scalet98\\AppData\\Local\\Temp/ipykernel_1476/196572691.py\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    ...\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# 5. forward propagation calculation in every time steps of each sample\n",
    "    ...\n",
    "    # calculating recurrent layer linear regression \n",
    "    for x in seq: \n",
    "        z1 = np.dot(x, self.w1x)+np.dot(self.h[-1], self.w1h) + self.b1\n",
    "        h = np.tanh(z1)                           # applying activation function \n",
    "        self.h.append(h)                          # saving hidden status for backpropagation \n",
    "        z2 = np.dot(h, self.w2) + self.b2         # calculating linear regression of output layer\n",
    "    return z2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5cbb3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Implementing backward propagation calculation \n",
    "\n",
    "def backpro(self, x, err): \n",
    "    m = len(x)              # no. of samples \n",
    "    \n",
    "    # calculating gradient of weight and interscept of output layer \n",
    "    w2_grad = np.dot(self.h[-1].T, err) / m\n",
    "    b2_grad = np.sum(err) / m \n",
    "    \n",
    "    # exchange batch dimension with time step dimension \n",
    "    seq = np.swapaxes(x, 0, 1)\n",
    "    \n",
    "    w1h_grad = w1x_grad = b1_grad = 0 \n",
    "    \n",
    "    # calculating gradient right before cell \n",
    "    err_to_cell = np.dot(err, self.w2.T) * (1-self.h[-1]**2)\n",
    "    \n",
    "    # transfer gradient by backpropagating every time step\n",
    "    for x, h in zip(seq[::-1][:10], self.h[:-1][::-1][:10]):\n",
    "        w1h_grad += np.dot(h.T, err_to_cell)\n",
    "        w1x_grad += np.dot(x.T, err_to_cell)\n",
    "        b1_grad += np.sum(err_to_cell, axis=0)\n",
    "        \n",
    "        err_to_cell = np.dot(err_to_cell, self.w1h) * (1-h**2)\n",
    "        \n",
    "    w1h_grad /= m\n",
    "    w1x_grad /= m\n",
    "    b1_grad /= m\n",
    "     \n",
    "    return w1h_grad, w1x_grad, b1_grad, w2_grad, b2_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6cc5ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Modifying other methods\n",
    "\n",
    "class RecurrentNetwork:\n",
    "    \n",
    "    def __init__(self, n_cells=10, batch_size=32, learning_rate=0.1):\n",
    "        self.n_cells = n_cells            # no. of cells\n",
    "        self.batch_size = batch_size      # batch size \n",
    "        self.w1h = None                   # weight of hidden status \n",
    "        self.w1x = None                   # weight of input \n",
    "        self.b1 = None                    # interscept of recurrent layer \n",
    "        self.w2 = None                    # weight of output layer\n",
    "        self.b2 = None                    # interscept of output layer\n",
    "        self.h = None                     # activation output of recurrent layer \n",
    "        self.losses = []                  # training loss \n",
    "        self.val_losses = []              # verification loss\n",
    "        self.lr = learning_rate           # learning rate \n",
    "        \n",
    "    def forpass(self, x): \n",
    "        self.h = [np.zeros((x.shape[0], self.n_cells))]    # initializing hidden status \n",
    "        # Exchange the batch dimension with step dimension \n",
    "        seq = np.swapaxes(x, 0, 1)\n",
    "        # calculating recurrent layer linear regression \n",
    "        for x in seq: \n",
    "            z1 = np.dot(x, self.w1x) + np.dot(self.h[-1], self.w1h) + self.b1\n",
    "            h = np.tanh(z1)                           # applying activation function \n",
    "            self.h.append(h)                          # saving hidden status for backpropagation \n",
    "            z2 = np.dot(h, self.w2) + self.b2         # calculating linear regression of output layer\n",
    "        return z2 \n",
    "    \n",
    "    def backprop(self, x, err): \n",
    "        m = len(x)              # no. of samples \n",
    "\n",
    "        # calculating gradient of weight and interscept of output layer \n",
    "        w2_grad = np.dot(self.h[-1].T, err) / m\n",
    "        b2_grad = np.sum(err) / m \n",
    "        # exchange batch dimension with time step dimension \n",
    "        seq = np.swapaxes(x, 0, 1)\n",
    "\n",
    "        w1h_grad = w1x_grad = b1_grad = 0 \n",
    "        # calculating gradient right before cell \n",
    "        err_to_cell = np.dot(err, self.w2.T) * (1-self.h[-1]**2)\n",
    "        # transfer gradient by backpropagating every time step\n",
    "        for x, h in zip(seq[::-1][:10], self.h[:-1][::-1][:10]):\n",
    "            w1h_grad += np.dot(h.T, err_to_cell)\n",
    "            w1x_grad += np.dot(x.T, err_to_cell)\n",
    "            b1_grad += np.sum(err_to_cell, axis=0)\n",
    "            # calculating gradient right before the cell in previous time step\n",
    "            err_to_cell = np.dot(err_to_cell, self.w1h) * (1-h**2)\n",
    "\n",
    "        w1h_grad /= m\n",
    "        w1x_grad /= m\n",
    "        b1_grad /= m\n",
    "\n",
    "        return w1h_grad, w1x_grad, b1_grad, w2_grad, b2_grad\n",
    "    \n",
    "    def sigmoid(self, z): \n",
    "        a = 1/ (1+np.exp(-z))                 # calculating sigmoid \n",
    "        return a \n",
    "    \n",
    "    def init_weights(self, n_features, n_classes): \n",
    "        orth_init = tf.initializers.Orthogonal()\n",
    "        glorot_init = tf.initializers.GlorotUniform()\n",
    "        \n",
    "        self.w1h = orth_init((self.n_cells, self.n_cells)).numpy() # (no. of cells, no. of cells)\n",
    "        self.w1x = glorot_init((n_features, self.n_cells)).numpy() # (no. of features, no. of cells)\n",
    "        self.b1 = np.zeros(self.n_cells)                           # size of hidden layer  \n",
    "        self.w2 = glorot_init((self.n_cells, n_classes)).numpy()  # (no. of cells, no. of classes)\n",
    "        self.b2 = np.zeros(n_classes)\n",
    "        \n",
    "    def fit(self, x, y, epochs=100, x_val=None, y_val=None):\n",
    "        y = y.reshape(-1, 1)\n",
    "        y_val = y_val.reshape(-1, 1)\n",
    "        np.random.seed(42)\n",
    "        self.init_weights(x.shape[2], y.shape[1])    # initializing weight of hidden layer and output layer \n",
    "        # repeate the routine epoch times \n",
    "        for i in range(epochs): \n",
    "            print (\"EPOCH\", i , end=' ')\n",
    "            # circulating mini-batch returned from generator function \n",
    "            batch_losses = []\n",
    "            for x_batch, y_batch in self.gen_batch(x, y): \n",
    "                print ('.', end ='')\n",
    "                a = self.training(x_batch, y_batch)    \n",
    "                \n",
    "                # clipping for safe logarian calculation \n",
    "                a = np.clip(a, 1e-10, 1-1e-10)\n",
    "                # accumulating logarian loss and regularization loss to the loss and adding it to the list\n",
    "                loss = np.mean(-(y_batch*np.log(a)+(1-y_batch)*np.log(1-a)))\n",
    "                batch_losses.append(loss)\n",
    "            print ( )\n",
    "            self.losses.append(np.mean(batch_losses))\n",
    "            # calculating the loss of verification set \n",
    "            self.update_val_loss(x_val, y_val)\n",
    "    \n",
    "    # mini-batch generator function \n",
    "    def gen_batch(self, x, y): \n",
    "        length = len(x)\n",
    "        bins = length // self.batch_size      # mini batch frequency \n",
    "        if length % self.batch_size: \n",
    "            bins +=1                          # if it has remainder        \n",
    "        indexes = np.random.permutation(np.arange(len(x)))   # shuffle indexes \n",
    "        x = x[indexes]\n",
    "        y = y[indexes]\n",
    "        for i in range(bins):\n",
    "            start = self.batch_size * i \n",
    "            end = self.batch_size * (i + 1)\n",
    "            yield x[start:end], y[start:end]      # returning the value slicing in batch_size                                \n",
    "        \n",
    "    \n",
    "    def training(self, x, y):\n",
    "        m = len(x)                            # saving the no. of samples\n",
    "        z = self.forpass(x)                   # carring on forward propagation calculation \n",
    "        a = self.sigmoid(z)                   # applying activation function \n",
    "        err = -(y - a)                        # calculating errors \n",
    "        # Calculating gradient by backpropagating errors\n",
    "        w1h_grad, w1x_grad, b1_grad, w2_grad, b2_grad = self.backprop(x, err)\n",
    "        # updating weight and interscept of cell \n",
    "        self.w1h -= self.lr * w1h_grad \n",
    "        self.w1x -= self.lr * w1x_grad \n",
    "        self.b1 -= self.lr * b1_grad\n",
    "        # updating weight and interscept of output layers \n",
    "        self.w2 -= self.lr *  w2_grad \n",
    "        self.b2 -= self.lr * b2_grad\n",
    "        return a \n",
    "        \n",
    "    def predict(self, x):\n",
    "        z = self.forpass(x)            # carry opn forward propagation calculation \n",
    "        return z > 0                   # applying step function \n",
    "    \n",
    "    def score(self, x, y):\n",
    "        # returning True ratio by comparing prediction with target column vector\n",
    "        return np.mean(self.predict(x) == y.reshape(-1, 1))    \n",
    "    \n",
    "    def update_val_loss(self, x_val, y_val):\n",
    "        z = self.forpass(x_val)           # carry opn forward propagation calculation \n",
    "        a = self.sigmoid(z)               # applying activation function \n",
    "        a = np.clip(a, 1e-10, 1-1e-10)    # clipping the output value \n",
    "        val_loss = np.mean(-(y_val*np.log(a) + (1-y_val)*np.log(1-a)))\n",
    "        self.val_losses.append(val_loss)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051b7d94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c298667",
   "metadata": {},
   "source": [
    "## # Model Training of Recurrent Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54eed7e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "EPOCH 1 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "EPOCH 2 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "EPOCH 3 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "EPOCH 4 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "EPOCH 5 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "EPOCH 6 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "EPOCH 7 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "EPOCH 8 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "EPOCH 9 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "EPOCH 10 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "EPOCH 11 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 12 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "EPOCH 13 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "EPOCH 14 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "EPOCH 15 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "EPOCH 16 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "EPOCH 17 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "EPOCH 18 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "EPOCH 19 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n"
     ]
    }
   ],
   "source": [
    "# 1. Model Training of Recurrent Network \n",
    "\n",
    "rn = RecurrentNetwork(n_cells=32, batch_size=32, learning_rate=0.01)\n",
    "\n",
    "rn.fit(x_train_onehot, y_train, epochs=20, x_val=x_val_onehot, y_val=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7fcc6aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. Plotting the Loss graph on the training and verifying set\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(rn.losses)\n",
    "plt.plot(rn.val_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1cffb1ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Evaluating accuracy of verification set \n",
    "\n",
    "rn.score(x_val_onehot, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2790a2e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow_py37",
   "language": "python",
   "name": "tf_py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
